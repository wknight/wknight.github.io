{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CARP seq2seq",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ogqi6NjBiTFh",
        "outputId": "358d503b-31f5-49b6-845b-72bc88b3246a"
      },
      "source": [
        "!pip install git+https://github.com/finetuneanon/transformers@gpt-neo-localattention3-rp-b"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/finetuneanon/transformers@gpt-neo-localattention3-rp-b\n",
            "  Cloning https://github.com/finetuneanon/transformers (to revision gpt-neo-localattention3-rp-b) to /tmp/pip-req-build-noohzkty\n",
            "  Running command git clone -q https://github.com/finetuneanon/transformers /tmp/pip-req-build-noohzkty\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (4.8.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (0.0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (0.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (0.0.46)\n",
            "Requirement already satisfied: einops==0.3.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (0.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.6.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.6.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm3OEe4loNVM",
        "outputId": "ef091c53-4270-45a8-f2b3-5344ef0934cd"
      },
      "source": [
        "!apt-get install unzip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-21ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgDD7JAhn-Ek",
        "outputId": "5ad6d556-2722-4993-e30b-d6ed4a2d311a"
      },
      "source": [
        "!wget http://the-eye.eu/public/AI/carp_seq2seq.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-15 19:37:35--  http://the-eye.eu/public/AI/carp_seq2seq.zip\n",
            "Resolving the-eye.eu (the-eye.eu)... 162.213.130.242\n",
            "Connecting to the-eye.eu (the-eye.eu)|162.213.130.242|:80... failed: Connection timed out.\n",
            "Retrying.\n",
            "\n",
            "--2021-10-15 19:39:46--  (try: 2)  http://the-eye.eu/public/AI/carp_seq2seq.zip\n",
            "Connecting to the-eye.eu (the-eye.eu)|162.213.130.242|:80... "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s7d6ZQXemmC"
      },
      "source": [
        "#We're using Neo for this tutorial\n",
        "from transformers import GPTNeoModel, GPTNeoForCausalLM,\\\n",
        "    GPT2Tokenizer, GPTNeoConfig\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raMo-PiudPmV"
      },
      "source": [
        "!unzip carp_seq2seq.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BftUqCKL1Iks"
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaY2N7cxcxQx"
      },
      "source": [
        "import torch\n",
        "from transformers import GPTNeoForCausalLM, AutoConfig\n",
        "\n",
        "# GPT-J 6B config\n",
        "config = AutoConfig.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "config.attention_layers = [\"global\"] * 28\n",
        "config.attention_types = [[\"global\"], 28]\n",
        "config.num_layers = 28\n",
        "config.num_heads = 16\n",
        "config.hidden_size = 256 * config.num_heads\n",
        "config.vocab_size = 50400\n",
        "config.rotary = True\n",
        "config.rotary_dim = 64\n",
        "config.jax = True\n",
        "\n",
        "try:\n",
        "    from collections.abc import MutableMapping\n",
        "except ImportError:\n",
        "    from collections import MutableMapping\n",
        "\n",
        "class Checkpoint(MutableMapping):\n",
        "    def __init__(self):\n",
        "        self.checkpoint = torch.load(\"j6b_ckpt/m.pt\", map_location=\"cpu\")\n",
        "    def __len__(self):\n",
        "        return len(self.checkpoint)\n",
        "    def __getitem__(self, key):\n",
        "        return torch.load(self.checkpoint[key], map_location=\"cpu\")\n",
        "    def __setitem__(self, key, value):\n",
        "        return\n",
        "    def __delitem__(self, key, value):\n",
        "        return\n",
        "    def keys(self):\n",
        "        return self.checkpoint.keys()\n",
        "    def __iter__(self):\n",
        "        for key in self.checkpoint:\n",
        "            yield (key, self.__getitem__(key))\n",
        "    def __copy__(self):\n",
        "        return Checkpoint()\n",
        "    def copy(self):\n",
        "        return Checkpoint()\n",
        "\n",
        "model = GPTNeoForCausalLM.from_pretrained(pretrained_model_name_or_path=None, config=config, state_dict=Checkpoint())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSInCw5gdbJ9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ1VXCiMdbaz"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6JTEzdrgnHX"
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "tokenizer.add_tokens([\"[QUOTE]\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNhSx2q4kaIb"
      },
      "source": [
        "%%capture\n",
        "model = model.half().to(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZZArAp7tU40"
      },
      "source": [
        "start_story = \"Tom wanted a sandwich.\"\n",
        "start_crit = \"I think after Tom gets a sandwich, you should express how full he is.\"\n",
        "start_example = \"Passage: \" + start_story + \"\\nCritique: \" + start_crit + \"\\n<|endoftext|>\\n\"\n",
        "\n",
        "\n",
        "story = \"\"\"Mornings I brough water for Lieutenant Awn to bath in, and dressed her, though the local costume was a good deal less effort than her uniform, and she had stopped wearing any sort of cosmetics two years before, as they were difficult to maintain in the heat.\"\"\"\n",
        "input_string = \"Passage: \" + story + \"\\nCritique: \"\n",
        "\n",
        "critique_start = \"I think before this happens, the character should \"\n",
        "\n",
        "toks = tokenizer.encode_plus(start_example + input_string + critique_start, return_tensors=\"pt\").to(\"cuda\")\n",
        "quote_idx = len(tokenizer)-1\n",
        "\n",
        "model.config.max_length=2048\n",
        "out = model.generate(**toks, num_beams=4, repetition_penalty=1.2, no_repeat_ngram_size=4, early_stopping=True, min_length=len(toks['input_ids'][0])+30, bad_words_ids=[[18559], [59], [quote_idx]])\n",
        "print(tokenizer.decode(out[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BB6nUdv1V4jQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}